---
title: "Electricity Demand Forecasting - ARIMA"
author: "Arunav Saikia"
output: 
  github_document: default
---

<h3> Load Data </h3>
```{r}
library(fpp2)
plot(elecdaily)
data <- data.frame(elecdaily)
# data <- data.frame(elecdemand)
Y <- data$Demand - mean(data$Demand) # output variable
demand <- Y
x <- data$Temperature - mean(data$Temperature) # input variable
```
<h3> Formulate Noise Models </h3>
Model 1 - $$y(t) = b_0 + b_1 · y(t − 1) + b_2 · y(t-2) + b_3 · y(t-3) + b_4 · x(t) + b_5 · x(t − 1) + b_6 · x(t - 2) + b_7 · cos(2\pi f_1t) \\+ b_8 · sin(2\pi f_1t) + b_9 · cos(2\pi f_2t) + b_{10} · sin(2\pi f_2t) + b_{11} · w(t-1) + b_{12} · w(t-2) + b_{13} · w(t-3) + w(t) $$
Model 2 - $$y(t) = b_0 + b_1 · y(t − 1) + b_2 · y(t-2) + b_3 · y(t-3) + b_4 · x(t) + b_5 · x(t − 1) + b_6 · x(t - 2) + b_7 · cos(2\pi f_1t) \\+ b_8 · sin(2\pi f_1t) + b_9 · cos(2\pi f_2t) + b_{10} · sin(2\pi f_2t) + b_{11} · w(t-1) + w(t) $$
Model 3 - $$y(t) = b_0 + b_1 · y(t − 1) + b_2 · x(t) + b_3 · x(t − 1) + b_4 · x(t - 2) + b_5 · cos(2\pi f_1t) \\+ b_6 · sin(2\pi f_1t) + b_7 · cos(2\pi f_2t) + b_{8} · sin(2\pi f_2t) + b_{9} · w(t-1) + b_{10} · w(t-2) + b_{11} · w(t-3) + w(t) $$
Model 4 - $$y(t) = b_0 + b_1 · y(t − 1) + b_2 · y(t-2) + b_3 · y(t-3) + b_4 · x(t) + b_5 · x(t − 1) + b_6 · cos(2\pi f_1t) \\+ b_7 · sin(2\pi f_1t) + b_8 · cos(2\pi f_2t) + b_{9} · sin(2\pi f_2t) + b_{10} · w(t-1) + w(t) $$
Model 5 - $$y(t) = b_0 + b_1 · y(t − 1) + b_2 · y(t-2)  + b_3 · x(t) + b_4 · x(t − 1) + b_5 · cos(2\pi f_1t) \\+ b_6 · sin(2\pi f_1t) + b_7 · cos(2\pi f_2t) + b_{8} · sin(2\pi f_2t) + b_{9} · cos(2\pi f_3t) + b_{10} · sin(2\pi f_3t) + b_{11} · w(t-1) + w(t) $$

<h3> Fit models using ARMAX </h3>
```{r}
Y <- data$Demand - mean(data$Demand) # output variable
x <- data$Temperature - mean(data$Temperature) # input variable

N = length(Y)

f1 = .1428571 # in previous assignment we found that f1 and f2
f2 = .2857143 # are two frequencies where the output signal has seasonal trend

# residuals are autocorrelated at f3 frequency so included sinusoids 
# to take make sure residuals are white noise
f3 = .429333333 

time = 1:N
cos1 = cos(2*pi*time*f1)[4:N]
sin1 = sin(2*pi*time*f1)[4:N]
cos2 = cos(2*pi*time*f2)[4:N]
sin2 = sin(2*pi*time*f2)[4:N]
cos3 = cos(2*pi*time*f3)[4:N]
sin3 = sin(2*pi*time*f3)[4:N]

dY_1 = filter(Y,c(0,1),sides=1)
dY_2 = filter(Y,c(0,0,1),sides=1)
dY_3 = filter(Y,c(0,0,0,1),sides=1)
dx_1 = filter(x,c(0,1),sides=1)
dx_2 = filter(x,c(0,0,1),sides=1)
dx_3 = filter(x,c(0,0,0,1),sides=1)

Y = Y[4:N]
x = x[4:N]
dY_1 = dY_1[4:N]
dY_2 = dY_2[4:N]
dY_3 = dY_3[4:N]
dx_1 = dx_1[4:N]
dx_2 = dx_2[4:N]
dx_3 = dx_3[4:N]

# while fitting the model we are looking at first 300 datapoints
# and we will use the future timepoints for forecasting

X1 = cbind(x,dx_1,dx_2,cos1,sin1,cos2,sin2)
res1 = arima(Y[1:300], xreg=X1[1:300,], order=c(3,0,3)) 
res1
X2 = cbind(x,dx_1,dx_2,cos1,sin1,cos2,sin2)
res2 = arima(Y[1:300], xreg=X2[1:300,], order=c(3,0,1)) 
res2
X3 = cbind(x,dx_1,dx_2,cos1,sin1,cos2,sin2)
res3 = arima(Y[1:300], xreg=X3[1:300,], order=c(1,0,3)) 
res3
X4 = cbind(x,dx_1,cos1,sin1,cos2,sin2)
res4 = arima(Y[1:300], xreg=X4[1:300,], order=c(3,0,1)) 
res4
X5 = cbind(x,dx_1,cos1,sin1,cos2,sin2, cos3, sin3)
res5 = arima(Y[1:300], xreg=X5[1:300,], order=c(2,0,1)) 
res5
```
<h3> Compare the models using BIC and AIC and Chi square (if nested) </h3>
```{r}
AIC(res1,res2,res3, res4, res5)

BIC(res1,res2,res3, res4, res5)
```
<h3>Choose the best model of those you compared. Justify your choice.</h3>
The best model is Model 5 which has lowest AIC and BIC scores
<h3> Examine the residuals of the model to see if they are white noise using
autocovariance on the residuals and specral analysis of the residuals.</h3>
```{r}
dRes = res5$residuals
# hRes1 = residuals(res1)
plot.ts(dRes)

acf(dRes, 30, main='')
Box.test(dRes,lag = 10, type = 'Box-Pierce')

S = spectrum(dRes, log="no")
```

The residuals are centered around zero and variance looks constant. The auto-correlation plot also does not show correlation at different lag components. From Box-Pierce test we can say that there is no significant evidence of auto-correlation upto lag 10 components. The periodogram looks flat with no significant peaks suggesting no autocovariance at different frequencies. Thus the residuals are white noise.

<h3> Interpretation of model parameters</h3>
```{r}
res5
```

The model has order 2 feedback terms and order 1 noise component. The  AR(2) term and MA(1) term are significant and positive. AR(1) is insignificant as 95% confidence interval of the coeff will include 0.
The model also has exogenous inputs which drive the mean of the output. There are sinuoids which capture cyclic/seasonal trends. Almost all sinusoids are statistically significant except cos3. $x(t)$ and $x(t-1)$ capture the effect of systematic input i.e temperature on the output. Since the coefficients are positive it means there is a positive correlation between temperature and electricity demand.
<h3> Compare Fitted Values to Observed values </h3>
```{r}
plot(Y[1:300], type = 'l', col = 'red', xlab = 'time', ylab = 'elec demand' )
lines(Y[1:300]-res5$residuals,col="green")
legend(230, 100, legend=c("Observed", "Fitted"),
       col=c("red", "green"), lty = 1:1, cex=0.8)

# cat("MAPE of fitted data is", mean(abs((Y[1:300]-(Y[1:300]-res5$residuals))/Y[1:300])))
accuracy(f=Y[1:300]-res5$residuals, x=Y[1:300])
```
<h3> Plot predictions of the model for future values </h3>
```{r}
yh=predict(res5,newxreg=X5[301:362,])
plot(Y[301:362], type = 'l', col = 'red', xlab = 'time', ylab = 'elec demand' )
lines(as.numeric(yh$pred),col="green")
legend(50, 20, legend=c("Observed", "Predicted"),
       col=c("red", "green"), lty = 1:1, cex=0.8)

accuracy(f=as.numeric(yh$pred), x= Y[301:362])
```
